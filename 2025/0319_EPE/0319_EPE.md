I'm reading [The Elements of Statistical Learning, 2ed](https://hastie.su.domains/ElemStatLearn/), p. 20, which deals with *classification* in machine learning.
The author have a definition of EPE (expected prediction error) as follows

$$\text{EPE}=\mathbb E\left[L\left(G,\hat G(X)\right)\right]\tag{2.19}$$

and proceed to get

$$\text{EPE}=\mathbb E_X\left[\sum_{j=1}^JL\left(g_j,\hat G(X)\right)P(G=g_j|X)\right]\tag{2.20}$$

by conditioning on $X$.
(I modified some notation from the original text.)
And I'm curious about the derivation from (2.19) to (2.20)

Here, two random variables $X\in\mathcal X$ and $G\in\mathcal G$ are called the predictor and the response, respectively, where $G$ is discrete.
$L$ is a loss function defined by

$$L(p,q)=1-\delta_{pq}=
\begin{cases}
0&(p=q)\\
1&(p\ne q)
\end{cases}$$

and a joint distribution of $X$ and $G$ is given. (Or, a dataset $\{(x^{(k)},g^{(k)}):1\le k\le K\}$ may be given, but even in this case, we can think of the corresponding joint distribution.)

For the case when the predictor is discrete, I can derive (2.20) from (2.19) easily as follow.
Suppose that

$$
\begin{align*}
\mathcal X&=\{x_1,\cdots,x_I\}=\{x_i:1\le i\le I\}\\
\mathcal G&=\{g_1,\cdots,g_J\}=\{g_j:1\le j\le J\}.
\end{align*}
$$

Then,

$$
\begin{align*}
\text{EPE}
&=\sum_{i=1}^I\sum_{j=1}^JL\left(g_j,\hat G(x_i)\right)\mathbb P\left(X=x_i, G=g_i\right)\\
&=\sum_{i=1}^I\left(\sum_{j=1}^JL\left(g_j,\hat G(x_i)\right)\mathbb P\left(G=g_i|X=x_i\right)\right)P_X\left(X=x_i\right)\\
% &=\sum_{i=1}^I\mathbb E_{G|X}\left[L\left(G,\hat G(x_i)\right)\right]P_X\left(X=x_i\right)\\
% &=\mathbb E_X\left[\mathbb E_{G|X}\left[L\left(G,\hat G(X)\right)\right]\right]\\
&=\mathbb E_X\left[\sum_{j=1}^JL\left(g_j,\hat G(x_i)\right)\mathbb P\left(G=g_i|X\right)\right].
\end{align*}
$$

But how can I derive (2.20) when $X$ is continuous ($\mathcal X\subset\mathbb R$) or $X$ is a random vector ($\mathcal X\subset\mathbb R^l$)?